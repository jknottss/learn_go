Еще одним приемом в аспекте
производительности, про которую стоит рассказать,
является поточная обработка данных. Давайте рассмотрим следующий пример. У нас есть какой-то XML, там есть какое-то количество полей,
и мы хотим его распарсить. Как это делается в лоб? Мы создаем у себя структуру Users, куда будем все парсить,
в которой есть элемент List и User. Тут все просто и понятно. Далее мы парсим это в лоб: мы
указываем все наши данные, которые у нас есть,
и указываем, куда их парсить. То есть мы мало того,
что загружаем в память весь slice byte, в котором содержится наш XML,
то есть тратим память на него, плюс мы тратим память на все структуры,
которые там есть. Соответственно, если у нас будет
очень большой объем данных, очень большой XML, либо json,
либо какая-то другая структура, мы не сможем загрузить его,
скорее всего, весь в память. И, вообще, держать все в
памяти — не всегда эффективно, потому что чаще всего мы проходимся
по этому циклу, в цикле. Соответственно, если мы хотим получить
доступ только к одной записи одновременно, то они все нам не нужны,
нам стоит тогда парсить их по порядку, одну за другой, и работать с одной
записью, не загружая в память сразу все. Посмотрим, как это делается. Вот есть следующий пример. Мы создаем уже декодер, мы создаем ридер, которые читает из слайса byte. Создаем декодер, куда передаем наш ридер. И все. А дальше мы пошли уже парсить
записи одну за другой. Мы получаем токен, смотрим, что если
там не конец файла, мы идем дальше. Мы также смотрим: ага,
нам пришел пришло начало элемента, то есть строка, в данном случае login. Я покажу, вот так она выглядит, то есть
нам пришел вот этот элемент, login. Мы смотрим по имени, что это за элемент. И теперь мы уже можем
декодировать именно его. Мы даже можем не парсить
всю структуру вообще, мы можем вытащить только одну строчку. Соответственно, я декодирую
только один элемент. Это гораздо более
эффективный способ парсинга. Собственно, это SAX parser,
который там есть. Подобные приемы есть для json — там
тоже есть свой ридер, там тоже можно организовывать поточный парсинг,
не пытаясь загрузить сразу все в память. Посмотрим, насколько это быстрее. Так, и декодер. Ага, обратите внимание: по скорости
это вышло почти в два раза быстрее, то есть количество записей,
которые удалось прогнать, для полного парсинга — 5 тысяч,
для декодера — 10. При этом декодер значительно быстрее
по скорости выполнения — он тратит меньше памяти на одну операцию и
делает гораздо меньше аллокаций. То есть поточный парсинг — это выгодно. Вы можете читать кусками данных с диска,
вы можете читать данные кусками из сети. И вам не придется загружать все в память. Этот подход очень часто используется
для разного рода числодробилок, которые обрабатывают
большие объемы данных.