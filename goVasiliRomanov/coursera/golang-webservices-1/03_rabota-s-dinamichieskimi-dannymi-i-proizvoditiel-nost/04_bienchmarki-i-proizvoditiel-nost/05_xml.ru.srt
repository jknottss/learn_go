1
00:00:00,000 --> 00:00:09,365
Еще одним приемом в аспекте
производительности,

2
00:00:09,365 --> 00:00:13,690
про которую стоит рассказать,
является поточная обработка данных.

3
00:00:13,690 --> 00:00:16,810
Давайте рассмотрим следующий пример.

4
00:00:16,810 --> 00:00:21,010
У нас есть какой-то XML,

5
00:00:21,010 --> 00:00:27,639
там есть какое-то количество полей,
и мы хотим его распарсить.

6
00:00:27,639 --> 00:00:30,730
Как это делается в лоб?

7
00:00:30,730 --> 00:00:35,610
Мы создаем у себя структуру Users,

8
00:00:35,610 --> 00:00:40,490
куда будем все парсить,
в которой есть элемент List и User.

9
00:00:40,490 --> 00:00:43,950
Тут все просто и понятно.

10
00:00:43,950 --> 00:00:49,126
Далее мы парсим это в лоб: мы
указываем все наши данные,

11
00:00:49,126 --> 00:00:53,215
которые у нас есть,
и указываем, куда их парсить.

12
00:00:53,215 --> 00:00:57,760
То есть мы мало того,
что загружаем в память весь slice byte,

13
00:00:57,760 --> 00:01:04,237
в котором содержится наш XML,
то есть тратим память на него,

14
00:01:04,237 --> 00:01:09,580
плюс мы тратим память на все структуры,
которые там есть.

15
00:01:09,580 --> 00:01:14,292
Соответственно, если у нас будет
очень большой объем данных,

16
00:01:14,292 --> 00:01:19,120
очень большой XML, либо json,
либо какая-то другая структура,

17
00:01:19,120 --> 00:01:22,706
мы не сможем загрузить его,
скорее всего, весь в память.

18
00:01:22,706 --> 00:01:25,559
И, вообще, держать все в
памяти — не всегда эффективно,

19
00:01:25,559 --> 00:01:28,751
потому что чаще всего мы проходимся
по этому циклу, в цикле.

20
00:01:28,751 --> 00:01:35,289
Соответственно, если мы хотим получить
доступ только к одной записи одновременно,

21
00:01:35,289 --> 00:01:41,122
то они все нам не нужны,
нам стоит тогда парсить их по порядку,

22
00:01:41,122 --> 00:01:47,550
одну за другой, и работать с одной
записью, не загружая в память сразу все.

23
00:01:47,550 --> 00:01:50,728
Посмотрим, как это делается.

24
00:01:50,728 --> 00:01:53,560
Вот есть следующий пример.

25
00:01:53,560 --> 00:01:58,970
Мы создаем уже декодер, мы создаем ридер,

26
00:01:58,970 --> 00:02:02,054
которые читает из слайса byte.

27
00:02:02,054 --> 00:02:04,860
Создаем декодер, куда передаем наш ридер.

28
00:02:04,860 --> 00:02:05,712
И все.

29
00:02:05,712 --> 00:02:10,840
А дальше мы пошли уже парсить
записи одну за другой.

30
00:02:10,840 --> 00:02:16,603
Мы получаем токен, смотрим, что если
там не конец файла, мы идем дальше.

31
00:02:16,603 --> 00:02:21,639
Мы также смотрим: ага,
нам пришел пришло начало элемента,

32
00:02:21,639 --> 00:02:24,580
то есть строка, в данном случае login.

33
00:02:24,580 --> 00:02:30,056
Я покажу, вот так она выглядит, то есть
нам пришел вот этот элемент, login.

34
00:02:30,056 --> 00:02:33,040
Мы смотрим по имени, что это за элемент.

35
00:02:33,040 --> 00:02:38,030
И теперь мы уже можем
декодировать именно его.

36
00:02:38,030 --> 00:02:41,771
Мы даже можем не парсить
всю структуру вообще,

37
00:02:41,771 --> 00:02:44,960
мы можем вытащить только одну строчку.

38
00:02:44,960 --> 00:02:51,930
Соответственно, я декодирую
только один элемент.

39
00:02:51,930 --> 00:02:54,920
Это гораздо более
эффективный способ парсинга.

40
00:02:54,920 --> 00:02:59,147
Собственно, это SAX parser,
который там есть.

41
00:02:59,147 --> 00:03:03,997
Подобные приемы есть для json — там
тоже есть свой ридер, там тоже можно

42
00:03:03,997 --> 00:03:08,140
организовывать поточный парсинг,
не пытаясь загрузить сразу все в память.

43
00:03:08,140 --> 00:03:13,073
Посмотрим, насколько это быстрее.

44
00:03:13,073 --> 00:03:15,390
Так, и декодер.

45
00:03:15,390 --> 00:03:21,275
Ага, обратите внимание: по скорости
это вышло почти в два раза быстрее,

46
00:03:21,275 --> 00:03:24,080
то есть количество записей,
которые удалось прогнать,

47
00:03:24,080 --> 00:03:28,208
для полного парсинга — 5 тысяч,
для декодера — 10.

48
00:03:28,208 --> 00:03:33,540
При этом декодер значительно быстрее
по скорости выполнения — он тратит

49
00:03:33,540 --> 00:03:39,680
меньше памяти на одну операцию и
делает гораздо меньше аллокаций.

50
00:03:39,680 --> 00:03:43,067
То есть поточный парсинг — это выгодно.

51
00:03:43,067 --> 00:03:50,510
Вы можете читать кусками данных с диска,
вы можете читать данные кусками из сети.

52
00:03:50,510 --> 00:03:53,620
И вам не придется загружать все в память.

53
00:03:53,620 --> 00:03:59,114
Этот подход очень часто используется
для разного рода числодробилок,

54
00:03:59,114 --> 00:04:01,980
которые обрабатывают
большие объемы данных.