[БЕЗ_ЗВУКА] В этом видео мы поговорим про балансировку
нагрузки и сервис Discovery. Это два очень актуальных
вопроса в мире микросервисов. Дело в том, что микросервисы часто
запускаются на большом количестве нод для обеспечения высокой доступности,
соответственно эти ноды могут удаляться, добавляться и каким-то образом вашему
софту, вашему основному сервису нужно узнавать, куда вообще можно
пускать нагрузку, нужно узнавать, что какой-то сервис ушел,
либо какая-то нода появилась. В этом видео мы будем рассматривать
оба этих вопроса на примере консула. Консул — это довольно мощная штука для
организации как раз сервиса Discovery, health-чеков, балансировки нагрузки,
key-value хранилища, распределенных логов. Я не буду очень глубоко
углубляться в его работу, потому что все-таки у
нас курс не по консулу. Однако я рассмотрю, как с практической
точки зрения можно его использовать. Кстати, консул тоже написан на Go. У меня он поднят в доке контейнере,
который, кстати, тоже написан на Go доке. Итак, у него есть небольшая админка,
где можно посмотреть какой-то статус нод, какие они вообще есть,
какие сервисы там зарегистрированы. Сейчас, как видно, в нем зарегистрирован
только сам консул, то есть ничего нет. И поэтому начнем мы с того,
что мы зарегистрируем в нем наш микросервис авторизации, который мы
рассматривали в предыдущих примерах. Теперь рассмотрим код. Консул — это внешний пакет,
его нужно поставить через Go get. Поскольку мне нужно будет
запустить сразу несколько нод его, все-таки это сервис Discovery,
там нужно, чтобы они уходили, приходили, я сразу сделаю его конфигурируемым,
используя пакет flag. У меня есть порт и также сразу
порт консула, он тоже доступен для конфигурирования, потому что вы можете
захотеть его поднять на другом порту. Итак. Начинаем мы нашу программу с того, что парсим непосредственно флаги,
которые передаются вот так вот. До этого я могу их просто объявить,
но автоматически они никак не распарсятся. После этого я привожу к строке,
просто для удобства, итак, я создаю... TCP здесь нет, создаю grpc-сервис,
регистрирую там мой сервис. В целом у меня практически нет отличий,
за исключением двух. Первое отличие: я добавил
в менеджер сессий порт, чтобы в логе можно было видеть,
на какой сервис мы пришли, потому что grpc в штатном режиме
это не позволяет это делать. И также я добавил заглушку,
потому что у меня не настоящий сервис авторизации,
а там просто хранится все в мапке. Соответственно, между собой эти
два сервиса никак не контактируют, поэтому там просто заглушка сейчас. Итак, я создаю config для консула,
пытаюсь к нему подключиться, после этого я задаю сервис ID. Я использую у себя просто какой-то
префикс, IP-адрес и порт, ну просто чтобы не идентифицировать. На самом деле ID там может быть
любой: MD5, UUID, что угодно. Далее я обращаюсь к консулу и пытаюсь зарегистрировать там свой сервис,
обращаюсь к агенту, вызываю функцию зарегистрировать
сервис и передаю туда мою структуру, в которой все будет зарегистрировано. Итак, ID — это как раз идентификатор
конкретно этого сервиса. Name — это то, какой сервис он
предоставляет, то есть ID должен быть уникальным в рамках разных нод,
а вот Name в рамках ноды одного кластера одних и тех
же микросервисов должен быть одинаковым. Указываю порт и указываю адрес,
на котором он доступен. У меня все это крутится локально,
поэтому 127.0.0.1. Я не указываю никаких health-чеков, у
меня сервис сразу же становится доступен. Однако вам, конечно же,
нужно будет их настроить. Вообще сразу оговорюсь,
что это все-таки не продакшн-пример, а какая-то отправная точка. Так, хорошо мы зарегистрировали сервер,
отлично. Теперь я сразу же через
отложенное выполнение defer регистрирую функцию, которая этот
сервис у меня выведет из нагрузки, точнее она сообщит консулу о том,
что сервис ушел. Таким образом, когда программа
закончится более-менее штатно, выполнится этот defer,
и он скажет консулу, что, пожалуйста, на этот сервер
слать нагрузки больше не надо. Так, теперь я стартую непосредственно grpc-сервис и уже ну просто жду,
чтобы остановить его. Итак, запустим первый сервис. Он зарегистрирован вот под таким ID,
стартуем и теперь ждем остановки. Так, посмотрим теперь сам консул,
ага, смотрите, появилось сервис наш session-api. session-api — это как раз
кластер таких микросервисов, в нем всего один всего одна нода. Там практически ничего нет,
если посмотреть, вот она есть: IP и порт. Вот. Теперь я добавлю еще один сервис. Мы его заказывали в отдельном терминале. Отлично. Так, стартовало. Смотрим. Ага, вот у меня теперь два
сервиса с session-api. Теперь я могу пускать туда нагрузку
и смотреть, как она распределяется. Так. Вот, поскольку мы подняли уже оба клиента, мы подняли уже оба наших микросервиса, теперь нам нужно сделать нагрузку
и получение в списке серверов их онлайн-обновления уже в клиенте,
который будет обращаться к этим серверам. Итак, что у меня тут есть? У меня по-прежнему есть консул api,
адрес консула конфигурируемый, клиент консулу и Resolver
я сделал глобальными, чтобы обращаться к ним из внешних функций, однако вы можете их
внедрить в свою структуру. Так. Ну по-прежнему мы подключаемся к консулу, теперь дальше, теперь мы получаем уже,
обращаясь к сервису health-чеков, мы говорим ему, дай мне,
пожалуйста сервисы session-api и дальнейшие параметры я не передаю
никакие, однако там можно указать теги, можно указать сервисы, которые только
прошли проверку, и еще некоторые вещи. Дело в том, что у меня сервис,
поскольку нет никаких health-чеков, он у меня сразу стартует живым, сразу
стартует здоровым, доступным для нагрузки. В продакшне так делать, конечно же,
не надо, все-таки лучше, чтобы там сначала health-чеки все прошли,
чтобы убедиться, что там все действительно работает и мы нигде не ошиблись
в ноде и в ее конфигурации, или какие-нибудь теги или фильтрации. Но здесь я получаю все. Что я дальше делаю? Дальше я по всем этим сервисам итерируюсь
и просто создаю из них слайс адресов. Дело в том,
что из консула мне возвращается структура, где есть отдельно IP, отдельно адрес. Тут я это все совмещаю. Теперь я создаю структуру
NameResolver и указываю там только нулевой адрес,
чтобы что-то там было в самом начале. После этого я уже создаю
подключение к grpc, я оказываю опцию WithBlock, это значит, что соединение, вот эта вот функция, она мне не вернет
результат, не вернет connect grpc, пока не будет установлено хотя
бы одно соединение с сервисом. И так же я передаю еще одну опцию,
которая говорит, что нужно использовать балансер между
разными нодами, то есть нод будет много. В качестве балансера я
использую RoundRobin. В grpc политика такая, что вот у вас даже
есть простой балансер типа RoundRobin, а если вы хотите какой-то более сложно
балансер, например, со взвешенным весами или что-то еще, то будьте добры реализуйте
это в виде отдельного внешнего сервиса. Ну в данном примере мы будем
рассматривать RoundRobin. И туда мы передаем нашу
структуру на NameResolver, я ее взял из тестового пакета самого grpc. Это структура будет сообщать
нашему балансеру о том, что нужно добавить ноду в список серверов,
либо убрать какую-то ноду. Итак, хорошо, мы вроде как начали,
создаем соединение, все хорошо. Закрываем его при выходе из программы. Теперь, если у меня вдруг было больше
одного сервера мне вернулось из консула, то я буду как раз использовать
NameResolver, я буду их туда добавлять. Ну тут все просто, я создаю слайс
и итерируюсь по всем сервисам и добавляю туда определенного
рода структуру. То есть я добавляю туда структуру update,
которая говорит, что нужно что-то с этим сделать. сделать в списке серверов
уже у соединения grpc. Вот там есть операция Add. Ну помимо Add, конечно,
я буду использовать Delete потом. И адрес,
в который нужно провести операцию. Ну и потом я, собственно,
выполняю свое обновление. Хорошо, то есть лично мы
создали соединение grpc, мы добавили туда все адреса, теперь оно
коннектится к разным нашим нодам, к двум. Дальше, я создаю sessManager
(вы его видели уже), я запускаю в отдельной
горутине обновление, то есть я буду опрашивать consul,
на предмет того, были ли там уже какие-то обновления, пришли ли какие-то новые сервисы,
нужно ли что-то удалить или добавить. К этой функции мы вернемся после. Далее я буду просто пытаться
проверить там какую-то сессию. Я не буду ее там создавать и проверять,
потому что я напомню, что у меня сейчас мои микросервисы
друг с другом никак не связаны, там просто отдельные автономные
map'ки внутри каждого, поэтому я буду просто запрашивать туда
несуществующий какой-то id-шник сессии, и на той стороне мне он
будет возвращать заглушку. Ну и потом я буду спать. Я делаю это в бесконечном цикле,
просто чтобы показать вам, что нагрузка идет и что она добавляется
и удаляется с разных сервисов. Сейчас я это продемонстрирую. Так, запускаем. Ага. Вот пошла нагрузка. Обратите внимание,
нагрузка у нас идет равномерно. Сначала идет ports 82, 83, 82, 83. Ну потому что у меня RoundRobin,
он по кругу идет по всем серверам, которые у него доступны. Теперь давайте посмотрим, каким образом будет происходить
удаление сервисов либо же добавление. Как раз функция runOnlineServiceDiscovery. Итак, что я тут делаю? Для начала я строю map'ку
из текущих адресов. С map'кой просто удобно работать. Далее я создаю ticker. Напомню, что этот ticker не имеет функцию
stop, и если эта функция завершится, то у меня будет утечка ресурсов. Однако я не планирую, что она будет
завершаться, и просто пускаю цикл по нему. Там идет чтение из канала, поэтому раз в
пять секунд у меня будет перечитывание. Хорошо, далее я фактически
обращаюсь уже к consul'у, использую уже знакомую
вам функцию Service, которая получает service-api
и получает текущие адреса. Хорошо. Теперь я из этих адресов
также строю map'у. Ну и дальше я итерируюсь
по обоим map'ам и смотрю, добавился ли какой-то сервис,
либо же какой-то сервис удалился. В данном случае тот способ, которым я опрашиваю consul,
это называется polling. Я периодически просто обращаюсь
к нему и говорю: есть что-то? Но там есть и другие варианты,
их мы рассмотрим отдельно. Итак, то есть раз в пять секунд
я буду обращаться к consul'у, брать все адреса, которые там есть,
сверять с тем, что есть у меня, и в зависимости от того,
удалилось ли там что-то, либо же добавилось там что-то,
я буду совершать разную операцию. Посмотрим, как это работает. Опять, запустим наш сервис,
который проверяет сессии. Теперь я потушу. Да, обратите внимание: 1, 3, 5. Вот идут его проверки. Теперь я потушу этот сервис. При завершении работы программы,
он отправил в consul информацию о том, что «я все, мне, пожалуйста,
слать ничего не надо». И consul это принял. Посмотрим. Тут у меня было два сервиса, смотрим. Теперь остался один. Вернемся в нашу программу. Ага, вот он. remove 127.0.0.1:8082. То есть наш микросервис был
успешно удален из списка. И обратите внимание,
теперь идет только на 83-м порту запрос. Теперь я запущу еще раз. Ага, теперь на 82-м порту, и я его поднял,
и на него опять пошли запросы. Смотрим программу. add. Пожалуйста, и я вычитал из
consul'а информацию о том, что там был добавлен какой-то сервис
и добавил его в список серверов, на который идет нагрузка, и,
соответственно, нагрузка туда пошла. Вот. То есть это довольно удобно. Соответственно, сервис discovery и
балансировка нагрузки — это тема очень большая, очень широкая, но я надеюсь, что
этот пример послужит вам хорошей отправной точкой того, каким образом это можно
реализовать: либо со стороны grpc, либо со стороны самого consul'а.